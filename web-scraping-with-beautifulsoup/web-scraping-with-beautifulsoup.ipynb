{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautiful Soup\n",
    "\n",
    "## Overview\n",
    "\n",
    "### What You'll Learn\n",
    "In this section, you'll learn\n",
    "1. How to scrape data from web pages like Wikipedia\n",
    "2. How to work with the data you scrape\n",
    "\n",
    "### Prerequisites\n",
    "Before starting this section, you should have an understanding of\n",
    "1. Requests add link\n",
    "\n",
    "### Introduction\n",
    "Web scraping allows you to parse through and work with data you find on public websites. We'll work through how to scrape data from Wikipedia, and you'll build a command-line tool that allows you to scrape Wikipedia like so:\n",
    "\n",
    "```\n",
    "> python3 demo.py\n",
    "Harvey G. Stenger\n",
    "\n",
    "7th president of Binghamton University\n",
    "Incumbent\n",
    "Assumed office 2012\n",
    "Preceded by C. Peter Magrath\n",
    "\n",
    "\n",
    "Personal details\n",
    "Alma mater Cornell University (B.S. 1979)Massachusetts Institute of Technology (Ph.D. 1983)\n",
    "Profession Educator, academic administrator\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "## How Web Scraping Works\n",
    "Web scraping allows you to get data from a website in the case that it doesn't have an API. It involves systematically searching websites' HTML for data of interest.\n",
    "\n",
    "`BeautifulSoup` is a Python library that allows users to easily parse through HTML documents in search for data. In this section, we'll use Wikipedia as an example, and try to scrape information from the infoboxes you usually see on the side.\n",
    "\n",
    "**Harvey Stenger's Infobox:**\n",
    "\n",
    "[insert image here]\n",
    "\n",
    "\n",
    "### HTML Structure\n",
    "In order to understand how to webscrape, it's important to understand the structure of HTML documents. When you view a webpage, your browser is sending a request to the website and receiving an HTML file in return. Your browser then interprets the HTML for you and displays it to you in a human-readable form.\n",
    "\n",
    "Every piece of information in an HTML document resides inside of an *element*. These elements come as `<div>`s, `<p>`s, `<tr>`s, and many more, and they're all nested within each other inside of HTML documents. The arragement of these elements as well as their individual properties tell the browser how to display them to the user.\n",
    "\n",
    "Ultimately, *we're not interested in the way the webpage looks* -- we're interested in the text data within each element, because that data is what we're trying to scrape.\n",
    "\n",
    "Here's some example HTML for a simple website (about web scraping):\n",
    "```html\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scraping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Web Scraping in Python</h1>\n",
    "        \n",
    "        <div id=\"requirements\">\n",
    "            <h2>Requirements:</h1>\n",
    "            <p>Python 3</p>\n",
    "            <p>requests library</p>\n",
    "            <p>BeautifulSoup library</p>\n",
    "        </div>\n",
    "        \n",
    "        <div id=\"how-to-scrape\">\n",
    "            <h2>How to Scrape:</h1>\n",
    "            <p>Load the HTML with requests</p>\n",
    "            <p>Pass the text of the request into BeautifulSoup</p>\n",
    "            <p>Use .find() or .find_all() functions to search for elements</p>\n",
    "        </div>\n",
    "        \n",
    "        <h4>Copyright HackBU 2019</h4>\n",
    "        \n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "[Here's a link](http://htmlpreview.github.io/?https://github.com/HackBinghamton/Webscraping-APIsWorkshop/blob/master/web-scraping-with-beautifulsoup/example_webpage.html) to this page so that your browser can render it for you.\n",
    "\n",
    "### Finding What to Scrape\n",
    "Looking at the example HTML and the corresponding screenshot, it's fairly easy to pick out where in the HTML each piece of text comes from. *This isn't the case most of the time.*\n",
    "\n",
    "Usually, larger websites will have hundreds of elements on a single page, were most of the elements don't actually do anything except hold other elements. They'll have horrible, unintuitive names and reading the HTML itself won't get you very far.\n",
    "\n",
    "Enter *Inspect Element*. This tool lets you look through every element on a webpage. To access it, right click on any part of a webpage and select *Inspect Element*.\n",
    "\n",
    "**Let's say that we decided to scrape from this webpage the names of every requirement.**\n",
    "\n",
    "By hovering over different elements in the *Inspector* pane, we can highlight what sections of the webpage they relate to. In the case of this website, we can see that the requirements we're trying to scrape are all found in the `div` element with `id=\"requirements\"`\n",
    "\n",
    "![Example Page with Inspector](img/inspector.png)\n",
    "\n",
    "Knowing that the `<div>` element with `id=\"requirements\"` gives us almost everything we need to start scraping -- yet, we need a little bit more information on how to filter down to *just* the requirements elements themselves.\n",
    "\n",
    "Looking back at the HTML, we can see that the *Requirements* `<div>` holds a `<h1>` header element as well as three `<p>` elements. These `<p>` elements contain the data we're trying to scrape.\n",
    "\n",
    "Now, we're ready to scrape.\n",
    "\n",
    "## How to Scrape\n",
    "With `BeautifulSoup` we can scrape this above webpage and pick out particular information about it.\n",
    "\n",
    "There are two main steps to scraping: loading the HTML and searching it. First, we must *load* the HTML into a `BeautifulSoup` parser and then *search* the HTML with the parser to find where our data is.\n",
    "\n",
    "### 1. Loading the HTML\n",
    "For the purposes of this demo, we'll have already stored the HTML as a string in a variable (`html_text`). Usually, you'd use a `requests` call to get the HTTP response from your target website, then use the response's `.text` instance variable to get the HTML as a string.\n",
    "\n",
    "Then, we'll create a `BeautifulSoup` parser object that we'll use to scrape with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BeautifulSoup parser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load your HTML (in this case, copy pasted, but usually grabbed from a request)\n",
    "html_text = \"\"\"<html>\n",
    "    <head>\n",
    "        <title>Web Scraping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Web Scraping in Python</h1>\n",
    "        \n",
    "        <div id=\"requirements\">\n",
    "            <h2>Requirements:</h2>\n",
    "            <p>Python 3</p>\n",
    "            <p>requests library</p>\n",
    "            <p>BeautifulSoup library</p>\n",
    "        </div>\n",
    "        \n",
    "        <div id=\"how-to-scrape\">\n",
    "            <h2>How to Scrape:</h2>\n",
    "            <p>Load the HTML with requests</p>\n",
    "            <p>Pass the text of the request into BeautifulSoup</p>\n",
    "            <p>Use .find() or .find_all() functions to search for elements</p>\n",
    "        </div>\n",
    "        \n",
    "        <h4>Copyright HackBU 2019</h4>\n",
    "        \n",
    "    </body>\n",
    "</html>\"\"\"\n",
    "\n",
    "# Create and load a parser with the HTML\n",
    "parser = BeautifulSoup(html_text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Scraping the HTML\n",
    "Given that the information we're trying to scrape is in the `<div>` with `id=\"requirements\"` we can direct our parser to find that area of the webpage.\n",
    "\n",
    "The *`.find()`* method allows you to search a document for elements with specific properties, and returns the first matching result. If we wanted to search a parser for the first `<h1>` tag on a website, we could run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = parser.find(\"h1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display what the parser found, we can print the parser object `h1`. *Notice that printing the parser alone will include the HTML tags around it.* To exclude the tags and print only the data within the tag, we can use the `.text` instance variable of `h1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the returned parser object alone will print the tags as well\n",
    "print(h1)\n",
    "\n",
    "# By printing the .text variable of the parser, we can extract the data inside of the tags\n",
    "print(h1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to be more specific. Oftentimes, elements will have certain attributes assigned to them, such as `id`, `class`, or `title`. We can search for elements that have these properties set to certain values. Let's say we wanted to find an element on a webpage such as `<p class=\"data\">`. We could use the following code to find the first instance of it.\n",
    "\n",
    "```python\n",
    "p = parser.find(\"p\", {\"class\": \"data\"})\n",
    "```\n",
    "\n",
    "In our HTML, we notice that all of the requirements we're trying to scrape are stored inside of a `<div>` with `id=\"requirements\"`. To scrape this one section, we can run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_div = parser.find(\"div\", {\"id\": \"requirements\"})\n",
    "print(\"All data:\")\n",
    "print(requirements_div)\n",
    "\n",
    "print(\"Just text:\")\n",
    "print(requirements_div.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's progress! Now, we want to just grab the `<p>` elements from this `<div>`, since those contain the requirements we're talking about.\n",
    "\n",
    "Thankfully, the `.find_all()` method of `BeautifulSoup` parsers will return you a list of each match found in a document! For example, to find every instance of `<div>` inside of the document we could run this code:\n",
    "\n",
    "```python\n",
    "divs = parser.find_all(\"div\")\n",
    "```\n",
    "\n",
    "So, given that we scraped up the `requirements` div from earlier, we can now scrape that div for each `<p>` element inside of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = requirements_div.find_all(\"p\")\n",
    "\n",
    "print(requirements)\n",
    "\n",
    "# To access and print each element, use a for loop!\n",
    "for requirement in requirements:\n",
    "    print(requirement.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now scraped up the data we were after!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Project: Making a Wikipedia Scraper\n",
    "Wikipedia often has boxes on the right-hand side that contain general info about the subject of the page. These boxes are called 'infoboxes'. Here's Harvey Stenger's:\n",
    "\n",
    "![Harvey Stenger's Infobox](img/harveybox.png)\n",
    "\n",
    "Let's make a tool to scrape these infoboxes and print out the output of them like so:\n",
    "```\n",
    "Harvey G. Stenger\n",
    "\n",
    "7th president of Binghamton University\n",
    "Incumbent\n",
    "Assumed office 2012\n",
    "Preceded by C. Peter Magrath\n",
    "\n",
    "\n",
    "Personal details\n",
    "Alma mater Cornell University (B.S. 1979)Massachusetts Institute of Technology (Ph.D. 1983)\n",
    "Profession Educator, academic administrator\n",
    "```\n",
    "\n",
    "### 1. Loading the HTML\n",
    "Before we can scrape, we must get the HTML for a Wikipedia page of our choice.\n",
    "\n",
    "Use a `requests.get()` call to fetch the Wikipedia website of your choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating and Loading the Parser\n",
    "Before we start searching for the data, let's make sure to create an instance of a parser.\n",
    "\n",
    "*Remember that the format for instantiating a parser is* `parser = BeautifulSoup(<HTML string>, \"html.parser\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Finding Elements with `.find()` and `.find_all()`\n",
    "Now, we can start to scrape.\n",
    "\n",
    "Wikipedia infoboxes are consistently found as a `<table>` with `class=\"infobox\"` *(Sometimes, the infobox `class` string will contain other words, too, but we don't need to worry about them).*\n",
    "\n",
    "Use your parser to find the `<table>` with `class=\"infobox\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** Since every Wikipedia infobox has a different structure, we cannot predict what other elements may be inside of it. Thus, the \"scraping\" part is over, and we just need to print out the text information inside of this infobox.\n",
    "\n",
    "However, you may notice that printing it out straight doesn't look very good. Here's the Harvey Stenger infobox information printed straight-up:\n",
    "\n",
    "```\n",
    "Harvey G. Stenger7th president of Binghamton UniversityIncumbentAssumed office 2012Preceded byC. Peter Magrath\n",
    "Personal detailsAlma materCornell University (B.S. 1979)Massachusetts Institute of Technology (Ph.D. 1983)ProfessionEducator, academic administrator\n",
    "```\n",
    "\n",
    "Due to the variable quantity of rows in this `<table>` element, we'll just have to iterate through and see what we can and can't print."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Getting Text Information from Elements\n",
    "To print out the infobox, we'll need to:\n",
    "1. Grab all items of the infobox that are `<tr>`s\n",
    "2. Iterate through the `<tr>` elements of the infobox (`for element in <tr element list>`)\n",
    "3. Print their `.text` attributes (*if they have them*)\n",
    "\n",
    "**IMPORTANT:** Not every element will have a `.text` attribute. So, in order to check this, we'll need to check each item as we iterate through to see if it has the `.text` attribute so that we don't get errors.\n",
    "\n",
    "To check if an object has a certain attribute in Python, use the `hasattr()` function.\n",
    "\n",
    "If we wanted to see if a variable `element` has an attribute `text`, we can use `hasattr(element, \"text\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# Grab each <tr> element in the infobox\n",
    "\n",
    "# Iterate through each <tr> element\n",
    "\n",
    "    # Check if it has .text attribute\n",
    "\n",
    "    # Print it\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! If all goes well, you'll have a basic Wikipedia scraper. To get your output to match mine, try checking if each `<tr>` element has sub-elements and printing them.\n",
    "\n",
    "Once you have this working, feel free to improve it! Add the ability for users\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
